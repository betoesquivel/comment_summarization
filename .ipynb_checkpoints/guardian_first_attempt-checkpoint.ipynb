{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the comments HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50  comments found in first page.\n",
      "50  authors found in first page.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "url = \"http://www.theguardian.com/discussion/p/4fqc7\"\n",
    "r = requests.get(url)\n",
    "html = r.text\n",
    "\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "comments = soup.select(\".d-comment__main\")\n",
    "comment_authors = soup.select(\".d-comment__author\")\n",
    "print len (comments), \" comments found in first page.\"\n",
    "print len (comment_authors), \" authors found in first page.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract the comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': u'Cameron today: ', 'author': u'sqrl'}, {'text': u'\"The UK has had a health insurance scheme for quite a while now. It is called National Insurance..\"', 'author': u'toggy12'}, {'text': u'Apologies for the spelling errors cold shivering hands and predictive text do not combine well.', 'author': u'Kevin Mundy'}, {'text': u\"The state doesn't owe you anything - the system is enacted by law.\", 'author': u'gtegte'}, {'text': u'No. I think what he is saying is pretty clear, and pretty reasonable.', 'author': u'SpotOn'}, {'text': u\"When we have a real living wage, there will no longer need to be 'stupid tax credits'. Until then, people need a top up to support themselves, because the companies they work for, don't want to give people their dues.\", 'author': u'thea1mighty'}]\n"
     ]
    }
   ],
   "source": [
    "comments_dict = []\n",
    "parsed_comments = []\n",
    "parsed_authors = []\n",
    "for comment, author in zip(comments, comment_authors):\n",
    "    c = comment.select(\".d-comment__body p\")[0].text\n",
    "    a = author['title']\n",
    "    comments_dict.append({\"text\": c, \"author\": a})\n",
    "    parsed_comments.append(t)\n",
    "    parsed_authors.append(a)\n",
    "    \n",
    "print comments_dict[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master f6d96c1] Comment text markup (quotes inside comments) is not being extracted as text.\n",
      " 1 file changed, 13 insertions(+), 7 deletions(-)\n",
      "Counting objects: 3, done.\n",
      "Delta compression using up to 8 threads.\n",
      "Compressing objects: 100% (3/3), done.\n",
      "Writing objects: 100% (3/3), 984 bytes | 0 bytes/s, done.\n",
      "Total 3 (delta 2), reused 0 (delta 0)\n",
      "To git@github.com:betoesquivel/comment_summarization.git\n",
      "   12b786a..f6d96c1  master -> master\n"
     ]
    }
   ],
   "source": [
    "!git add -A && git commit -m \"Comment text markup (quotes inside comments) is not being extracted as text.\" && git push origin master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize all stemmed comments using tfidf\n",
    "We will do some stemming in the comment text in order to create shorter vectors that represent each comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk.stem\n",
    "\n",
    "english_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer=super(StemmedTfidfVectorizer,self).build_analyzer()\n",
    "        return lambda doc:(english_stemmer.stem(w) for w in analyzer(doc))\n",
    "\n",
    "stem_vectorizer = StemmedTfidfVectorizer(min_df=1, stop_words='english')\n",
    "stem_analyze = stem_vectorizer.build_analyzer()\n",
    "Y = []\n",
    "for c in parsed_comments:\n",
    "    Y.append(stem_analyze(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for y in Y:\n",
    "    tokens = [tok for tok in y]\n",
    "    print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
