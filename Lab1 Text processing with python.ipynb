{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic usage of Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1, 2],\n",
       "        [2, 3],\n",
       "        [3, 4],\n",
       "        [4, 5],\n",
       "        [5, 6]]), array([1, 2, 3, 4, 5]), array([2, 3, 4, 5, 6]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = np.array([[1,2], [2,3], [3,4], [4,5], [5,6]])\n",
    "x = data[:,0]\n",
    "y = data[:,1]\n",
    "\n",
    "data, x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text processing with Scikit learn\n",
    "We can use *CountVectorizer* to extract a bag of words representation from a collection of documents, using the SciKit-Learn method fit_transform. We will use a list of strings as documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([u'disk', u'format', u'hard', u'how', u'my', u'problems', u'to'],\n",
       " array([[1, 1, 1, 1, 1, 0, 1],\n",
       "        [1, 1, 1, 0, 0, 1, 0]]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df = 1)\n",
    "content = [\"How to format my hard disk\", \" Hard disk format problems \"]\n",
    "\n",
    "# fit_transform returns array of two rows, one per 'document'.\n",
    "# each row has 7 elements, each element being the number of items\n",
    "# a given feature occurred in that document.\n",
    "X = vectorizer.fit_transform(content)\n",
    "\n",
    "vectorizer.get_feature_names(), X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Array vector for the first document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 0, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of times word \"hard\" occurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()[1][vectorizer.get_feature_names().index('hard')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the 20 Newsgroups dataset\n",
    "We are going to fetch just some categories so that it doesn't take that long to download the docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "categories = ['alt.atheism', 'soc.religion.christian',\n",
    "              'comp.graphics', 'sci.med']\n",
    "twenty_train = fetch_20newsgroups(subset='train',\n",
    "                                 categories=categories, shuffle=True,\n",
    "                                 random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a CountVectorizer object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "train_counts = vectorizer.fit_transform(twenty_train.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see how frequently the word **algorithm** occurs in the subset of the 20Newgroups collection we are considering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4690"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_.get(u'algorithm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many terms were extracted? use *get_feature_names()*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35788"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*CountVectorizer* can do more preprocessing. This can be stopword removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'across',\n",
       " 'after',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " 'all',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'am',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'amoungst']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "sorted(vectorizer.get_stop_words())[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More preprocessing\n",
    "For **stemming** and more advanced preprocessing, supplement SciKit Learn with another Python library, NLTK. Up next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More advanced preprocessing with NLTK\n",
    "NLTK is described in detail in a book by Bird, Klein and Loper available online:\n",
    "http://www.nltk.org/book_1ed/ for version 2.7 of python\n",
    "\n",
    "## About NLTK\n",
    "* It is not the best\n",
    "* It is very easy to use\n",
    "\n",
    "You should read the book linked above to get familiar with the package and with text preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an English stemmer\n",
    "http://www.nltk.org/howto/stem.html for general intro.\n",
    "http://www.nltk.org/api/nltk.stem.html for more details (including languages covered)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'cat', u'ran', u'jump')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = nltk.stem.SnowballStemmer('english')\n",
    "s.stem(\"cats\"), s.stem(\"ran\"), s.stem(\"jumped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK for text analytics\n",
    "* NERs\n",
    "* Sentiment analysis\n",
    "* Extracting information from social media."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "text = word_tokenize(\"And now for something completely different\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('And', 'CC'),\n",
       " ('now', 'RB'),\n",
       " ('for', 'IN'),\n",
       " ('something', 'NN'),\n",
       " ('completely', 'RB'),\n",
       " ('different', 'JJ')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrating NLTK with SciKit's vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK Stemmer\n",
    "The stemmer can be used to stem documents before feeding into SciKit's vectorizer, thus obtaining a more compact index.\n",
    "One way to do this is to define a new class *StemmedCountVectorizer* extending *CountVectorizer* by redifining the method *build_analyzer()* that handles preprocessing and tokenization.\n",
    "\n",
    "http://scikitlearn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "\n",
    "*build_analyzer()* takes a string as input and outputs a list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'john', u'bought', u'carrots', u'potatoes']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "analyze = vectorizer.build_analyzer()\n",
    "analyze(\"John bought carrots and potatoes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we modify build_analyzer() to apply the NLTK stemmer to the output of default build_analyzer(), we get a version that does stemming as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk.stem\n",
    "english_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer=super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc:(english_stemmer.stem(w) for w in analyzer(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we can create an instance of this class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'john', u'bought', u'carrot', u'potato']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem_vectorizer = StemmedCountVectorizer(min_df=1,\n",
    "                                        stop_words='english')\n",
    "stem_analyze = stem_vectorizer.build_analyzer()\n",
    "Y = stem_analyze(\"John bought carrots and potatoes\")\n",
    "\n",
    "[tok for tok in Y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use this vectorizer to extract features\n",
    "\n",
    "Compare this result to around 35,000 features we obtained using the unstemmed version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 21801)\t1\n",
      "  (0, 7414)\t4\n",
      "  (0, 3982)\t2\n",
      "  (0, 24860)\t2\n",
      "  (0, 16603)\t3\n",
      "  (0, 7664)\t3\n",
      "  (0, 23291)\t1\n",
      "  (0, 8048)\t3\n",
      "  (0, 13380)\t1\n",
      "  (0, 13029)\t2\n",
      "  (0, 15168)\t2\n",
      "  (0, 13343)\t2\n",
      "  (0, 17763)\t1\n",
      "  (0, 19575)\t1\n",
      "  (0, 13005)\t1\n",
      "  (0, 12402)\t1\n",
      "  (0, 18309)\t1\n",
      "  (0, 25122)\t2\n",
      "  (0, 15512)\t1\n",
      "  (0, 587)\t1\n",
      "  (0, 9528)\t1\n",
      "  (0, 14886)\t1\n",
      "  (0, 12005)\t1\n",
      "  (0, 26032)\t1\n",
      "  (0, 22970)\t1\n",
      "  :\t:\n",
      "  (5, 7893)\t1\n",
      "  (5, 8052)\t2\n",
      "  (5, 11738)\t1\n",
      "  (5, 10823)\t1\n",
      "  (5, 5559)\t1\n",
      "  (5, 4064)\t1\n",
      "  (5, 19573)\t1\n",
      "  (5, 21596)\t1\n",
      "  (5, 9606)\t1\n",
      "  (5, 22968)\t1\n",
      "  (5, 10061)\t1\n",
      "  (5, 10238)\t1\n",
      "  (5, 19197)\t1\n",
      "  (5, 12061)\t1\n",
      "  (5, 23254)\t1\n",
      "  (5, 21137)\t1\n",
      "  (5, 24451)\t1\n",
      "  (5, 25969)\t1\n",
      "  (5, 6408)\t1\n",
      "  (5, 13897)\t1\n",
      "  (5, 20641)\t1\n",
      "  (5, 9531)\t1\n",
      "  (5, 15677)\t1\n",
      "  (5, 14290)\t1\n",
      "  (5, 6821)\t1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "categories = ['alt.atheism', 'soc.religion.christian',\n",
    "             'comp.graphics', 'sci.med']\n",
    "twenty_train = fetch_20newsgroups(subset='train',\n",
    "                                 categories=categories,\n",
    "                                 shuffle=True, random_state=42)\n",
    "train_counts = stem_vectorizer.fit_transform(twenty_train.data)\n",
    "\n",
    "len(stem_vectorizer.get_feature_names())\n",
    "print train_counts[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "You should always experiment and see if it is good to use stemming with your problem set. It might not be the best thing to do.\n",
    "\n",
    "SOLR works for processing larger datasets, since Python and SciKit-Learn become less effective, and more industrial strength software is required. One example of such software is Apache SOLR, an open source indexing package available from: \n",
    "http://lucene.apache.org/solr/\n",
    "It produces Lucene-style indices that can be used by text analytics packages such as Mahout.\n",
    "\n",
    "Elastic http://www.elastic.co/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook Lab1 Text processing with python.ipynb to script\n",
      "[NbConvertApp] Writing 5943 bytes to Lab1 Text processing with python.py\n"
     ]
    }
   ],
   "source": [
    "!ipython nbconvert --to script Lab1\\ Text\\ processing\\ with\\ python.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
